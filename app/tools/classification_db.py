# app/tools/classification_db.py
# -*- coding: utf-8 -*-
"""
–ú–æ–¥—É–ª—å –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –≤ SQLite.
–ü–æ—Ç—Ä—ñ–±–Ω–æ –¥–ª—è –∞–≥–µ–Ω—Ç–∞-–ø–µ—Ä—Å–æ–Ω–∏, —â–æ–± –≤—ñ–Ω –º—ñ–≥ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —ñ—Å—Ç–æ—Ä–∏—á–Ω—ñ –¥–∞–Ω—ñ.
"""

import sqlite3
import json
from typing import List, Dict, Any, Optional
import pandas as pd

try:
    from logger import logger
except Exception:
    import logging
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")
    logger = logging.getLogger("classification_db")

def _ensure_database_schema(conn: sqlite3.Connection) -> None:
    """–°—Ç–≤–æ—Ä—é—î –≤—Å—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ —Ç–∞–±–ª–∏—Ü—ñ –¥–ª—è –±–æ—Ç–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—Å—Ç—ñ."""
    
    # –¢–∞–±–ª–∏—Ü—è analyses - —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –ø—Ä–æ–≤–µ–¥–µ–Ω—ñ –∞–Ω–∞–ª—ñ–∑–∏
    conn.execute("""
        CREATE TABLE IF NOT EXISTS analyses (
            analysis_id INTEGER PRIMARY KEY AUTOINCREMENT,
            video_id TEXT NOT NULL,
            created_at TEXT NOT NULL,
            model TEXT NOT NULL,
            total_comments INTEGER NOT NULL,
            used_comments INTEGER NOT NULL,
            fast_mode INTEGER NOT NULL DEFAULT 1
        )
    """)
    
    # –¢–∞–±–ª–∏—Ü—è comment_labels - —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–æ–º–µ–Ω—Ç–∞—Ä—è
    conn.execute("""
        CREATE TABLE IF NOT EXISTS comment_labels (
            comment_id TEXT NOT NULL,
            video_id TEXT NOT NULL,
            labels_json TEXT NOT NULL,  -- JSON array: ["praise", "suggestions"]
            top_label TEXT,             -- Top category: "praise"
            sentiment TEXT,             -- Sentiment: "positive", "neutral", "negative"
            analysis_id INTEGER,
            PRIMARY KEY (comment_id, video_id),
            FOREIGN KEY (comment_id) REFERENCES comments (comment_id),
            FOREIGN KEY (analysis_id) REFERENCES analyses (analysis_id)
        )
    """)
    
    # –¢–∞–±–ª–∏—Ü—è topics_summary - –∑–≤–µ–¥–µ–Ω–Ω—è —Ç–µ–º –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –≤—ñ–¥–µ–æ
    conn.execute("""
        CREATE TABLE IF NOT EXISTS topics_summary (
            video_id TEXT NOT NULL,
            topic_id TEXT NOT NULL,
            count INTEGER NOT NULL,
            share REAL NOT NULL,
            analysis_id INTEGER,
            PRIMARY KEY (video_id, topic_id),
            FOREIGN KEY (analysis_id) REFERENCES analyses (analysis_id)
        )
    """)
    
    # –¢–∞–±–ª–∏—Ü—è sentiment_summary - –∑–≤–µ–¥–µ–Ω–Ω—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –≤—ñ–¥–µ–æ
    conn.execute("""
        CREATE TABLE IF NOT EXISTS sentiment_summary (
            video_id TEXT NOT NULL,
            sentiment TEXT NOT NULL,    -- "positive", "neutral", "negative"
            count INTEGER NOT NULL,
            share REAL NOT NULL,        -- 0.0 - 1.0
            analysis_id INTEGER,
            PRIMARY KEY (video_id, sentiment),
            FOREIGN KEY (analysis_id) REFERENCES analyses (analysis_id)
        )
    """)
    
    # –û–Ω–æ–≤–ª–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü—è classification_results –¥–ª—è –∑–≤–æ—Ä–æ—Ç–Ω–æ—ó —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ
    conn.execute("""
        CREATE TABLE IF NOT EXISTS classification_results (
            comment_id TEXT PRIMARY KEY,
            video_id TEXT NOT NULL,
            topic_labels TEXT,  -- JSON array: ["praise", "suggestions"]
            topic_top TEXT,     -- Top category: "praise"
            confidence REAL,    -- Confidence score (0-1)
            classified_at TEXT, -- ISO timestamp
            model_used TEXT,    -- e.g. "openai/gpt-4o-mini"
            batch_size INTEGER, -- Batch size used for classification
            FOREIGN KEY (comment_id) REFERENCES comments (comment_id)
        )
    """)
    
    # –Ü–Ω–¥–µ–∫—Å–∏ –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –ø–æ—à—É–∫—É
    conn.execute("CREATE INDEX IF NOT EXISTS idx_analyses_video_id ON analyses(video_id)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_analyses_created_at ON analyses(created_at)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_comment_labels_video_id ON comment_labels(video_id)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_comment_labels_top_label ON comment_labels(top_label)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_topics_summary_video_id ON topics_summary(video_id)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_classification_video_id ON classification_results(video_id)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_classification_topic_top ON classification_results(topic_top)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_classification_classified_at ON classification_results(classified_at)")
    
    conn.commit()

def save_classification_results(
    df: pd.DataFrame, 
    sqlite_path: str,
    model_name: str = "openai/gpt-4o-mini",
    batch_size: int = 20
) -> bool:
    """
    –ó–±–µ—Ä—ñ–≥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –≤ SQLite.
    
    Args:
        df: DataFrame –∑ –∫–æ–ª–æ–Ω–∫–∞–º–∏ comment_id, video_id, topic_labels_llm, topic_top_llm
        sqlite_path: –®–ª—è—Ö –¥–æ SQLite —Ñ–∞–π–ª—É
        model_name: –ù–∞–∑–≤–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ
        batch_size: –†–æ–∑–º—ñ—Ä –±–∞—Ç—á—É, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–≤—Å—è
        
    Returns:
        bool: True —è–∫—â–æ —É—Å–ø—ñ—à–Ω–æ –∑–±–µ—Ä–µ–∂–µ–Ω–æ
    """
    if df.empty:
        logger.warning("–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó")
        return False
    
    try:
        with sqlite3.connect(sqlite_path) as conn:
            _ensure_database_schema(conn)
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
            rows = []
            for _, row in df.iterrows():
                labels = row.get("topic_labels_llm", [])
                labels_json = json.dumps(labels) if isinstance(labels, list) else "[]"
                
                rows.append({
                    "comment_id": str(row["comment_id"]),
                    "video_id": str(row.get("video_id", "")),
                    "topic_labels": labels_json,
                    "topic_top": row.get("topic_top_llm"),
                    "confidence": 1.0,  # TODO: –¥–æ–¥–∞—Ç–∏ —Ä–µ–∞–ª—å–Ω—É confidence –≤—ñ–¥ LLM
                    "classified_at": pd.Timestamp.utcnow().isoformat(),
                    "model_used": model_name,
                    "batch_size": batch_size
                })
            
            # Upsert –≤ –ë–î
            conn.executemany("""
                INSERT INTO classification_results 
                (comment_id, video_id, topic_labels, topic_top, confidence, classified_at, model_used, batch_size)
                VALUES 
                (:comment_id, :video_id, :topic_labels, :topic_top, :confidence, :classified_at, :model_used, :batch_size)
                ON CONFLICT(comment_id) DO UPDATE SET
                    topic_labels=excluded.topic_labels,
                    topic_top=excluded.topic_top,
                    confidence=excluded.confidence,
                    classified_at=excluded.classified_at,
                    model_used=excluded.model_used,
                    batch_size=excluded.batch_size
            """, rows)
            
            conn.commit()
            logger.info(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –¥–ª—è {len(rows)} –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –≤ {sqlite_path}")
            return True
            
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó: {e}")
        return False

def load_classification_results(
    video_id: str, 
    sqlite_path: str
) -> pd.DataFrame:
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –∑–±–µ—Ä–µ–∂–µ–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –¥–ª—è –≤—ñ–¥–µ–æ.
    
    Args:
        video_id: ID YouTube –≤—ñ–¥–µ–æ
        sqlite_path: –®–ª—è—Ö –¥–æ SQLite —Ñ–∞–π–ª—É
        
    Returns:
        DataFrame –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–±–æ –ø–æ—Ä–æ–∂–Ω—ñ–π DataFrame
    """
    try:
        with sqlite3.connect(sqlite_path) as conn:
            query = """
                SELECT 
                    c.comment_id, c.video_id, c.text, c.like_count, c.published_at,
                    c.author, c.lang,
                    cr.topic_labels, cr.topic_top, cr.confidence, 
                    cr.classified_at, cr.model_used, cr.batch_size
                FROM comments c
                LEFT JOIN classification_results cr ON c.comment_id = cr.comment_id
                WHERE c.video_id = ?
                ORDER BY c.like_count DESC
            """
            df = pd.read_sql_query(query, conn, params=[video_id])
            
            # –ü–∞—Ä—Å–∏–Ω–≥ JSON –¥–ª—è topic_labels
            if not df.empty and "topic_labels" in df.columns:
                df["topic_labels_llm"] = df["topic_labels"].apply(
                    lambda x: json.loads(x) if x and x != 'null' else []
                )
                df["topic_top_llm"] = df["topic_top"]
                
            logger.info(f"üìä –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –¥–ª—è –≤—ñ–¥–µ–æ {video_id}")
            return df
            
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó: {e}")
        return pd.DataFrame()

def get_topic_statistics(
    video_id: str, 
    sqlite_path: str
) -> Dict[str, Any]:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–µ–º –¥–ª—è –≤—ñ–¥–µ–æ.
    
    Args:
        video_id: ID YouTube –≤—ñ–¥–µ–æ
        sqlite_path: –®–ª—è—Ö –¥–æ SQLite —Ñ–∞–π–ª—É
        
    Returns:
        {"total_comments": int, "topics": [{"topic": str, "count": int, "share": float}]}
    """
    try:
        with sqlite3.connect(sqlite_path) as conn:
            # –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤
            total_query = "SELECT COUNT(*) as total FROM classification_results WHERE video_id = ?"
            total_result = conn.execute(total_query, [video_id]).fetchone()
            total_comments = total_result[0] if total_result else 0
            
            if total_comments == 0:
                return {"total_comments": 0, "topics": [], "video_id": video_id}
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–º
            topics_query = """
                SELECT 
                    topic_top as topic,
                    COUNT(*) as count,
                    ROUND(COUNT(*) * 1.0 / ? * 100, 1) as share_percent
                FROM classification_results 
                WHERE video_id = ? AND topic_top IS NOT NULL
                GROUP BY topic_top
                ORDER BY count DESC
            """
            topics_df = pd.read_sql_query(
                topics_query, 
                conn, 
                params=[total_comments, video_id]
            )
            
            topics = topics_df.to_dict('records')
            
            # –î–æ–¥–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –≤—ñ–¥–µ–æ
            video_info_query = """
                SELECT COUNT(*) as total_comments_in_db
                FROM comments 
                WHERE video_id = ?
            """
            video_info = conn.execute(video_info_query, [video_id]).fetchone()
            total_in_db = video_info[0] if video_info else 0
            
            return {
                "video_id": video_id,
                "total_comments": total_comments,
                "total_in_db": total_in_db,
                "classification_coverage": round(total_comments / total_in_db * 100, 1) if total_in_db > 0 else 0,
                "topics": topics
            }
            
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        return {"total_comments": 0, "topics": [], "video_id": video_id}

def get_video_list_with_classification(sqlite_path: str) -> pd.DataFrame:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –≤—Å—ñ—Ö –≤—ñ–¥–µ–æ –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—é.
    
    Returns:
        DataFrame –∑ –∫–æ–ª–æ–Ω–∫–∞–º–∏: video_id, total_comments, classified_comments, last_classified_at
    """
    try:
        with sqlite3.connect(sqlite_path) as conn:
            query = """
                SELECT 
                    c.video_id,
                    COUNT(c.comment_id) as total_comments,
                    COUNT(cr.comment_id) as classified_comments,
                    MAX(cr.classified_at) as last_classified_at,
                    cr.model_used
                FROM comments c
                LEFT JOIN classification_results cr ON c.comment_id = cr.comment_id
                GROUP BY c.video_id
                ORDER BY last_classified_at DESC NULLS LAST
            """
            df = pd.read_sql_query(query, conn)
            
            if not df.empty:
                df["classification_coverage"] = round(
                    df["classified_comments"] / df["total_comments"] * 100, 1
                )
            
            logger.info(f"üìã –ó–Ω–∞–π–¥–µ–Ω–æ {len(df)} –≤—ñ–¥–µ–æ –≤ –±–∞–∑—ñ –¥–∞–Ω–∏—Ö")
            return df
            
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É –≤—ñ–¥–µ–æ: {e}")
        return pd.DataFrame()

def delete_classification_results(
    video_id: Optional[str] = None, 
    sqlite_path: str = "./.cache.db"
) -> bool:
    """
    –í–∏–¥–∞–ª—è—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó.
    
    Args:
        video_id: ID –≤—ñ–¥–µ–æ –¥–ª—è –≤–∏–¥–∞–ª–µ–Ω–Ω—è (—è–∫—â–æ None - –≤–∏–¥–∞–ª—è—î –≤—Å—ñ)
        sqlite_path: –®–ª—è—Ö –¥–æ SQLite —Ñ–∞–π–ª—É
        
    Returns:
        bool: True —è–∫—â–æ —É—Å–ø—ñ—à–Ω–æ –≤–∏–¥–∞–ª–µ–Ω–æ
    """
    try:
        with sqlite3.connect(sqlite_path) as conn:
            if video_id:
                conn.execute("DELETE FROM classification_results WHERE video_id = ?", [video_id])
                logger.info(f"üóëÔ∏è –í–∏–¥–∞–ª–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –¥–ª—è –≤—ñ–¥–µ–æ {video_id}")
            else:
                conn.execute("DELETE FROM classification_results")
                logger.info("üóëÔ∏è –í–∏–¥–∞–ª–µ–Ω–æ –≤—Å—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó")
            
            conn.commit()
            return True
            
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è: {e}")
        return False

def save_analysis_to_db(
    video_id: str,
    total_comments: int, 
    used_comments: int,
    model_name: str,
    df_classified: pd.DataFrame,
    topics_summary: pd.DataFrame,
    sqlite_path: str,
    fast_mode: bool = True,
    sentiment_summary: Optional[pd.DataFrame] = None
) -> int:
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –ø–æ–≤–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –≤—ñ–¥–µ–æ –≤ –Ω–æ–≤–∏—Ö —Ç–∞–±–ª–∏—Ü—è—Ö.
    
    Args:
        video_id: ID YouTube –≤—ñ–¥–µ–æ
        total_comments: –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤
        used_comments: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–∏—Ö –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        model_name: –ù–∞–∑–≤–∞ LLM –º–æ–¥–µ–ª—ñ
        df_classified: DataFrame –∑ –∫–ª–∞—Å–∏—Ñ—ñ–∫–æ–≤–∞–Ω–∏–º–∏ –∫–æ–º–µ–Ω—Ç–∞—Ä—è–º–∏ (–º–∞—î –º—ñ—Å—Ç–∏—Ç–∏ 'sentiment')
        topics_summary: DataFrame –∑—ñ –∑–≤–µ–¥–µ–Ω–Ω—è–º —Ç–µ–º
        sqlite_path: –®–ª—è—Ö –¥–æ SQLite —Ñ–∞–π–ª—É
        fast_mode: –ß–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–≤—Å—è —à–≤–∏–¥–∫–∏–π —Ä–µ–∂–∏–º
        sentiment_summary: DataFrame –∑ –ø—ñ–¥—Å—É–º–∫–æ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ
        
    Returns:
        analysis_id: ID —Å—Ç–≤–æ—Ä–µ–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É
    """
    try:
        with sqlite3.connect(sqlite_path) as conn:
            _ensure_database_schema(conn)
            
            # 1. –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–ø–∏—Å –∞–Ω–∞–ª—ñ–∑—É
            created_at = pd.Timestamp.utcnow().isoformat()
            cursor = conn.execute("""
                INSERT INTO analyses (video_id, created_at, model, total_comments, used_comments, fast_mode)
                VALUES (?, ?, ?, ?, ?, ?)
            """, [video_id, created_at, model_name, total_comments, used_comments, int(fast_mode)])
            
            analysis_id = cursor.lastrowid
            
            # 2. –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –º—ñ—Ç–∫–∏ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤
            comment_labels = []
            for _, row in df_classified.iterrows():
                labels = row.get("topic_labels_llm", [])
                labels_json = json.dumps(labels) if isinstance(labels, list) else "[]"
                
                comment_labels.append({
                    "comment_id": str(row["comment_id"]),
                    "video_id": video_id,
                    "labels_json": labels_json,
                    "top_label": row.get("topic_top_llm"),
                    "sentiment": row.get("sentiment", "neutral"),  # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º neutral
                    "analysis_id": analysis_id
                })
            
            if comment_labels:
                conn.executemany("""
                    INSERT OR REPLACE INTO comment_labels 
                    (comment_id, video_id, labels_json, top_label, sentiment, analysis_id)
                    VALUES (:comment_id, :video_id, :labels_json, :top_label, :sentiment, :analysis_id)
                """, comment_labels)
            
            # 3. –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∑–≤–µ–¥–µ–Ω–Ω—è —Ç–µ–º
            topic_rows = []
            for _, row in topics_summary.iterrows():
                topic_rows.append({
                    "video_id": video_id,
                    "topic_id": row["topic_id"], 
                    "count": int(row["count"]),
                    "share": float(row["share"]),
                    "analysis_id": analysis_id
                })
            
            if topic_rows:
                # –°–ø–æ—á–∞—Ç–∫—É –≤–∏–¥–∞–ª—è—î–º–æ —Å—Ç–∞—Ä—ñ –¥–∞–Ω—ñ –¥–ª—è —Ü—å–æ–≥–æ –≤—ñ–¥–µ–æ
                conn.execute("DELETE FROM topics_summary WHERE video_id = ?", [video_id])
                
                conn.executemany("""
                    INSERT INTO topics_summary (video_id, topic_id, count, share, analysis_id)
                    VALUES (:video_id, :topic_id, :count, :share, :analysis_id)
                """, topic_rows)
            
            # 4. –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∑–≤–µ–¥–µ–Ω–Ω—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ (—è–∫—â–æ —î)
            sentiment_rows = []
            if sentiment_summary is not None:
                for _, row in sentiment_summary.iterrows():
                    sentiment_rows.append({
                        "video_id": video_id,
                        "sentiment": row["sentiment"],
                        "count": int(row["count"]),
                        "share": float(row["share"]),
                        "analysis_id": analysis_id
                    })
                
                if sentiment_rows:
                    conn.executemany("""
                        INSERT OR REPLACE INTO sentiment_summary 
                        (video_id, sentiment, count, share, analysis_id)
                        VALUES (:video_id, :sentiment, :count, :share, :analysis_id)
                    """, sentiment_rows)
            
            conn.commit()
            logger.info(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –∞–Ω–∞–ª—ñ–∑ #{analysis_id} –¥–ª—è –≤—ñ–¥–µ–æ {video_id}: {len(comment_labels)} –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤, {len(topic_rows)} —Ç–µ–º, {len(sentiment_rows)} —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π")
            return analysis_id
            
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∞–Ω–∞–ª—ñ–∑—É: {e}")
        return -1

def get_latest_analysis_data(video_id: str, sqlite_path: str) -> Dict[str, Any]:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î –¥–∞–Ω—ñ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –¥–ª—è –≤—ñ–¥–µ–æ —É —Ñ–æ—Ä–º–∞—Ç—ñ –¥–ª—è Telegram –±–æ—Ç–∞.
    
    Returns:
        {
            "analysis_id": int,
            "video_id": str,
            "total_comments": int,
            "used_comments": int,
            "model": str,
            "created_at": str,
            "topics": [{"topic_id": str, "name": str, "count": int, "share": float, "top_quote": str}],
            "classified_comments": DataFrame
        }
    """
    try:
        with sqlite3.connect(sqlite_path) as conn:
            # –û—Ç—Ä–∏–º—É—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π –∞–Ω–∞–ª—ñ–∑
            analysis_query = """
                SELECT * FROM analyses 
                WHERE video_id = ? 
                ORDER BY created_at DESC 
                LIMIT 1
            """
            analysis_df = pd.read_sql_query(analysis_query, conn, params=[video_id])
            
            if analysis_df.empty:
                return {"error": "–ù–µ–º–∞—î –∑–±–µ—Ä–µ–∂–µ–Ω–∏—Ö –∞–Ω–∞–ª—ñ–∑—ñ–≤ –¥–ª—è —Ü—å–æ–≥–æ –≤—ñ–¥–µ–æ"}
            
            analysis = analysis_df.iloc[0].to_dict()
            analysis_id = analysis["analysis_id"]
            
            # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–æ–ø —Ç–µ–º –∑ –∑–≤–µ–¥–µ–Ω–Ω—è
            topics_query = """
                SELECT * FROM topics_summary 
                WHERE video_id = ? 
                ORDER BY count DESC
            """
            topics_df = pd.read_sql_query(topics_query, conn, params=[video_id])
            
            # –û—Ç—Ä–∏–º—É—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ
            sentiment_query = """
                SELECT * FROM sentiment_summary 
                WHERE video_id = ? 
                ORDER BY count DESC
            """
            sentiment_df = pd.read_sql_query(sentiment_query, conn, params=[video_id])
            
            # –û—Ç—Ä–∏–º—É—î–º–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫–æ–≤–∞–Ω—ñ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ –∑ —Ü–∏—Ç–∞—Ç–∞–º–∏
            comments_query = """
                SELECT 
                    c.comment_id, c.text, c.like_count, c.published_at, c.author,
                    cl.labels_json, cl.top_label
                FROM comments c
                JOIN comment_labels cl ON c.comment_id = cl.comment_id
                WHERE cl.video_id = ?
                ORDER BY c.like_count DESC
            """
            comments_df = pd.read_sql_query(comments_query, conn, params=[video_id])
            
            # –û–±—Ä–æ–±–ª—è—î–º–æ —Ç–µ–º–∏ –∑ —Ü–∏—Ç–∞—Ç–∞–º–∏
            from .topics_taxonomy import ID2NAME
            topics_with_quotes = []
            
            for _, topic_row in topics_df.iterrows():
                topic_id = topic_row["topic_id"]
                topic_name = ID2NAME.get(topic_id, topic_id)
                
                # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–π–∫—Ä–∞—â—É —Ü–∏—Ç–∞—Ç—É –¥–ª—è —Ü—ñ—î—ó —Ç–µ–º–∏
                topic_comments = comments_df[comments_df["top_label"] == topic_id]
                top_quote = ""
                if not topic_comments.empty:
                    best_comment = topic_comments.iloc[0]
                    top_quote = str(best_comment["text"] or "")[:200]
                
                topics_with_quotes.append({
                    "topic_id": topic_id,
                    "name": topic_name,
                    "count": int(topic_row["count"]),
                    "share": float(topic_row["share"]),
                    "top_quote": top_quote
                })
            
            # –û–±—Ä–æ–±–ª—è—î–º–æ JSON –º—ñ—Ç–∫–∏ —É –∫–æ–º–µ–Ω—Ç–∞—Ä—è—Ö
            if not comments_df.empty and "labels_json" in comments_df.columns:
                comments_df["topic_labels_llm"] = comments_df["labels_json"].apply(
                    lambda x: json.loads(x) if x and x != 'null' else []
                )
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–ø–∏—Å–æ–∫ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π
            sentiment_names = {"positive": "–ü–æ–∑–∏—Ç–∏–≤–Ω–∞", "neutral": "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞", "negative": "–ù–µ–≥–∞—Ç–∏–≤–Ω–∞"}
            sentiment_emojis = {"positive": "üòä", "neutral": "üòê", "negative": "üòü"}
            sentiment_list = []
            
            for _, row in sentiment_df.iterrows():
                sentiment_id = row["sentiment"]
                sentiment_list.append({
                    "sentiment": sentiment_id,
                    "name": sentiment_names.get(sentiment_id, sentiment_id),
                    "emoji": sentiment_emojis.get(sentiment_id, ""),
                    "count": int(row["count"]),
                    "share": float(row["share"])
                })
            
            return {
                "analysis_id": analysis_id,
                "video_id": video_id,
                "total_comments": analysis["total_comments"],
                "used_comments": analysis["used_comments"],
                "model": analysis["model"],
                "created_at": analysis["created_at"],
                "topics": topics_with_quotes,
                "sentiment": sentiment_list,
                "classified_comments": comments_df
            }
            
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –∞–Ω–∞–ª—ñ–∑—É: {e}")
        return {"error": f"–ü–æ–º–∏–ª–∫–∞: {e}"}

def get_topic_quotes(video_id: str, topic_id: str, sqlite_path: str, limit: int = 3) -> List[Dict[str, Any]]:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î –Ω–∞–π–∫—Ä–∞—â—ñ —Ü–∏—Ç–∞—Ç–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó —Ç–µ–º–∏.
    
    Returns:
        [{"comment_id": str, "text": str, "author": str, "like_count": int}, ...]
    """
    try:
        with sqlite3.connect(sqlite_path) as conn:
            query = """
                SELECT 
                    c.comment_id, c.text, c.author, c.like_count, c.published_at
                FROM comments c
                JOIN comment_labels cl ON c.comment_id = cl.comment_id
                WHERE cl.video_id = ? AND cl.top_label = ?
                ORDER BY c.like_count DESC, c.published_at DESC
                LIMIT ?
            """
            df = pd.read_sql_query(query, conn, params=[video_id, topic_id, limit])
            
            quotes = []
            for _, row in df.iterrows():
                quotes.append({
                    "comment_id": row["comment_id"],
                    "text": str(row["text"] or ""),
                    "author": str(row["author"] or ""),
                    "like_count": int(row["like_count"] or 0)
                })
            
            return quotes
            
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Ü–∏—Ç–∞—Ç: {e}")
        return []

def get_filtered_comments(
    video_id: str,
    sqlite_path: str,
    topic_id: Optional[str] = None,
    sentiment: Optional[str] = None,
    limit: int = 10
) -> List[Dict[str, Any]]:
    """
    –û—Ç—Ä–∏–º—É—î –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ –∑ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—î—é –∑–∞ —Ç–µ–º–æ—é —Ç–∞/–∞–±–æ —Ç–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—é.
    
    Args:
        video_id: ID YouTube –≤—ñ–¥–µ–æ
        sqlite_path: –®–ª—è—Ö –¥–æ SQLite –ë–î  
        topic_id: –§—ñ–ª—å—Ç—Ä –∑–∞ —Ç–µ–º–æ—é (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
        sentiment: –§—ñ–ª—å—Ç—Ä –∑–∞ —Ç–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—é (positive/neutral/negative)
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        
    Returns:
        –°–ø–∏—Å–æ–∫ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –∑ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏
    """
    try:
        with sqlite3.connect(sqlite_path) as conn:
            # –ë—É–¥—É—î–º–æ WHERE —É–º–æ–≤–∏
            where_conditions = ["cl.video_id = ?"]
            params = [video_id]
            
            if topic_id:
                where_conditions.append("cl.top_label = ?")
                params.append(topic_id)
                
            if sentiment:
                where_conditions.append("cl.sentiment = ?")
                params.append(sentiment)
            
            where_clause = " AND ".join(where_conditions)
            
            query = f"""
                SELECT 
                    c.text, c.like_count, c.published_at, c.author,
                    cl.top_label, cl.sentiment, cl.labels_json
                FROM comments c
                JOIN comment_labels cl ON c.comment_id = cl.comment_id AND c.video_id = cl.video_id  
                WHERE {where_clause}
                ORDER BY c.like_count DESC, c.published_at DESC
                LIMIT ?
            """
            params.append(limit)
            
            df = pd.read_sql_query(query, conn, params=params)
            
            result = []
            for _, row in df.iterrows():
                # –ë–µ–∑–ø–µ—á–Ω–∞ –æ–±—Ä–æ–±–∫–∞ likes
                try:
                    likes = int(row["like_count"] or 0)
                except (ValueError, TypeError):
                    likes = 0
                    
                result.append({
                    "text": row["text"],
                    "likes": likes,
                    "author": row["author"] or "Unknown",
                    "date": row["published_at"],
                    "topic": row["top_label"],
                    "sentiment": row["sentiment"] or "neutral"
                })
            
            logger.info(f"üîç Found {len(result)} filtered comments (topic={topic_id}, sentiment={sentiment})")
            return result
            
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤: {e}")
        return []
