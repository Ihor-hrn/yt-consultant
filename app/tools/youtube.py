# app/tools/youtube.py
# -*- coding: utf-8 -*-
"""
YouTube comments fetcher (M1: Integration, P0)
- –û—Ç—Ä–∏–º—É—î –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ (–≤–∫–ª—é—á–Ω–æ –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥—è–º–∏) —á–µ—Ä–µ–∑ YouTube Data API v3
- –ü—ñ–¥—Ç—Ä–∏–º—É—î –ø–∞–≥—ñ–Ω–∞—Ü—ñ—é, "—à–≤–∏–¥–∫–∏–π —Ä–µ–∂–∏–º" —á–µ—Ä–µ–∑ max_comments/max_pages
- –ü–æ–≤–µ—Ä—Ç–∞—î pandas.DataFrame —ñ (–æ–ø—Ü—ñ–π–Ω–æ) –∫–µ—à—É—î —É SQLite
"""

from __future__ import annotations
import os
import re
import time
import sqlite3
from typing import Optional, Dict, Any, List, Tuple

import pandas as pd
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# ---------- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–º—ñ–Ω–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ –∑ .env ----------
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # –Ø–∫—â–æ python-dotenv –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ, —Å–ø—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ .env –≤—Ä—É—á–Ω—É
    env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')
    if os.path.exists(env_path):
        with open(env_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

# ---------- –õ–æ–≥–µ—Ä (–≤–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ –≤–∞—à logger, —è–∫—â–æ —î) ----------
try:
    from logger import logger  # –≤–∞—à —ñ—Å–Ω—É—é—á–∏–π –ª–æ–≥–µ—Ä
except Exception:  # fallback ‚Äî –±–∞–∑–æ–≤–∏–π
    import logging
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")
    logger = logging.getLogger("yt")

# ---------- –î–æ–ø–æ–º—ñ–∂–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó ----------
VIDEO_ID_RE = re.compile(
    r"""
    (?:youtu\.be/|youtube\.com/(?:watch\?v=|embed/|shorts/|live/))
    ([0-9A-Za-z_-]{11})
    """,
    re.VERBOSE,
)

def extract_video_id(url_or_id: str) -> Optional[str]:
    """
    –í–∏—Ç—è–≥—É—î 11-—Å–∏–º–≤–æ–ª—å–Ω–∏–π video_id –∑ –ø–æ–≤–Ω–æ–≥–æ URL –∞–±–æ –ø–æ–≤–µ—Ä—Ç–∞—î —Ä—è–¥–æ–∫, —è–∫—â–æ –≤—ñ–Ω —É–∂–µ —Å—Ö–æ–∂–∏–π –Ω–∞ id.
    –ü—ñ–¥—Ç—Ä–∏–º–∫–∞: youtu.be/, watch?v=, embed/, shorts/, live/
    """
    s = (url_or_id or "").strip()
    if not s:
        return None
    # –Ø–∫—â–æ –≤–∂–µ —Å—Ö–æ–∂–µ –Ω–∞ video_id
    if re.fullmatch(r"[0-9A-Za-z_-]{11}", s):
        return s

    # –°–ø–µ—Ä—à—É —Å–ø—Ä–æ–±—É—î–º–æ —á–µ—Ä–µ–∑ query –ø–∞—Ä–∞–º–µ—Ç—Ä v=
    if "watch?" in s and "v=" in s:
        from urllib.parse import urlparse, parse_qs
        q = parse_qs(urlparse(s).query)
        v = q.get("v", [None])[0]
        if v and re.fullmatch(r"[0-9A-Za-z_-]{11}", v):
            return v

    # –Ü–Ω–∞–∫—à–µ ‚Äî —á–µ—Ä–µ–∑ –∑–∞–≥–∞–ª—å–Ω—É —Ä–µ–≥—É–ª—è—Ä–∫—É
    m = VIDEO_ID_RE.search(s)
    return m.group(1) if m else None


def _ensure_sqlite(conn: sqlite3.Connection) -> None:
    """–°—Ç–≤–æ—Ä—é—î —Ç–∞–±–ª–∏—Ü—é comments –ø—Ä–∏ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—Å—Ç—ñ."""
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS comments (
            video_id TEXT NOT NULL,
            comment_id TEXT PRIMARY KEY,
            parent_id TEXT,
            author TEXT,
            author_channel_id TEXT,
            text TEXT,
            like_count INTEGER,
            reply_count INTEGER,
            published_at TEXT,
            updated_at TEXT,
            is_reply INTEGER,
            fetched_at TEXT
        )
        """
    )
    conn.commit()


def _upsert_comments(conn: sqlite3.Connection, rows: List[Dict[str, Any]]) -> None:
    """UPSERT —É SQLite (ON CONFLICT REPLACE –∑–∞ comment_id)."""
    if not rows:
        return
    _ensure_sqlite(conn)
    conn.executemany(
        """
        INSERT INTO comments (
            video_id, comment_id, parent_id, author, author_channel_id, text,
            like_count, reply_count, published_at, updated_at, is_reply, fetched_at
        )
        VALUES (
            :video_id, :comment_id, :parent_id, :author, :author_channel_id, :text,
            :like_count, :reply_count, :published_at, :updated_at, :is_reply, :fetched_at
        )
        ON CONFLICT(comment_id) DO UPDATE SET
            parent_id=excluded.parent_id,
            author=excluded.author,
            author_channel_id=excluded.author_channel_id,
            text=excluded.text,
            like_count=excluded.like_count,
            reply_count=excluded.reply_count,
            published_at=excluded.published_at,
            updated_at=excluded.updated_at,
            is_reply=excluded.is_reply,
            fetched_at=excluded.fetched_at
        """,
        rows,
    )
    conn.commit()


# ---------- –û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è ----------
def fetch_comments(
    url_or_id: str,
    *,
    api_key: Optional[str] = None,
    order: str = "relevance",           # 'relevance' | 'time'
    include_replies: bool = True,
    max_results_per_page: int = 100,    # 1..100
    max_pages: Optional[int] = None,    # –æ–±–º–µ–∂–µ–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–æ–∫
    max_comments: Optional[int] = None, # –æ–±–º–µ–∂–µ–Ω–Ω—è –∑–∞–≥–∞–ª—å–Ω–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ
    text_format: str = "plainText",     # 'plainText' | 'html'
    sleep_between_pages: float = 0.0,   # —Å–µ–∫. –º—ñ–∂ —Å—Ç–æ—Ä—ñ–Ω–∫–∞–º–∏ (—è–∫—â–æ –±–æ—ó—à—Å—è –∫–≤–æ—Ç)
    sqlite_path: Optional[str] = None,  # —è–∫—â–æ –∑–∞–¥–∞–Ω–æ ‚Äî –∑–±–µ—Ä–µ–∂–µ–º–æ –∫–µ—à
    quota_user: Optional[str] = None,   # —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä –¥–ª—è quota –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è
) -> pd.DataFrame:
    """
    –°—Ç—è–≥—É—î –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ –¥–ª—è –≤—ñ–¥–µ–æ. –ü–æ–≤–µ—Ä—Ç–∞—î DataFrame –∑—ñ —Å—Ç–æ–≤–ø—á–∏–∫–∞–º–∏:
    ['video_id','comment_id','parent_id','author','author_channel_id','text',
     'like_count','reply_count','published_at','updated_at','is_reply']
    """

    t0 = time.perf_counter()
    video_id = extract_video_id(url_or_id)
    if not video_id:
        raise ValueError("–ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏ video_id. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –∞–±–æ ID.")

    api_key = api_key or os.getenv("YOUTUBE_API_KEY")
    if not api_key:
        raise ValueError("–ù–µ –∑–∞–¥–∞–Ω–æ api_key —ñ –∑–º—ñ–Ω–Ω—É —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ YOUTUBE_API_KEY.")

    logger.info(f"üì• Fetch comments for video_id={video_id} (order={order})")

    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª—ñ—î–Ω—Ç–∞
    client_kwargs = {"developerKey": api_key}
    if quota_user:
        client_kwargs["quotaUser"] = quota_user  # –∫–æ—Ä–∏—Å–Ω–æ –¥–ª—è –∫–≤–æ—Ç
    youtube = build("youtube", "v3", **client_kwargs)

    rows: List[Dict[str, Any]] = []
    next_page_token: Optional[str] = None
    page = 0

    try:
        while True:
            if max_pages is not None and page >= max_pages:
                logger.info(f"‚èπÔ∏è max_pages={max_pages} –¥–æ—Å—è–≥–Ω—É—Ç–æ")
                break

            page += 1
            t_page = time.perf_counter()
            request = youtube.commentThreads().list(
                part="snippet,replies",
                videoId=video_id,
                maxResults=max_results_per_page,
                order=order,
                pageToken=next_page_token,
                textFormat=text_format,  # –ø–æ–≤–µ—Ä—Ç–∞—î plainText —É textDisplay
            )
            response = request.execute()
            items = response.get("items", [])
            fetched_this_page = 0

            for item in items:
                thread_id = item.get("id")
                snip = item["snippet"]
                top = snip["topLevelComment"]["snippet"]

                # top-level
                rows.append({
                    "video_id": video_id,
                    "comment_id": item["snippet"]["topLevelComment"]["id"],
                    "parent_id": None,
                    "author": top.get("authorDisplayName"),
                    "author_channel_id": (top.get("authorChannelId") or {}).get("value"),
                    "text": top.get("textDisplay") or top.get("textOriginal"),
                    "like_count": int(top.get("likeCount", 0) or 0),
                    "reply_count": int(snip.get("totalReplyCount", 0) or 0),
                    "published_at": top.get("publishedAt"),
                    "updated_at": top.get("updatedAt"),
                    "is_reply": 0,
                    "fetched_at": pd.Timestamp.utcnow().isoformat(),
                })
                fetched_this_page += 1

                if include_replies and "replies" in item:
                    for reply in item["replies"].get("comments", []):
                        rs = reply["snippet"]
                        rows.append({
                            "video_id": video_id,
                            "comment_id": reply["id"],
                            "parent_id": thread_id,
                            "author": rs.get("authorDisplayName"),
                            "author_channel_id": (rs.get("authorChannelId") or {}).get("value"),
                            "text": rs.get("textDisplay") or rs.get("textOriginal"),
                            "like_count": int(rs.get("likeCount", 0) or 0),
                            "reply_count": 0,
                            "published_at": rs.get("publishedAt"),
                            "updated_at": rs.get("updatedAt"),
                            "is_reply": 1,
                            "fetched_at": pd.Timestamp.utcnow().isoformat(),
                        })
                        fetched_this_page += 1

                # –û–±–º–µ–∂–µ–Ω–Ω—è –ø–æ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ
                if max_comments is not None and len(rows) >= max_comments:
                    logger.info(f"‚èπÔ∏è –î–æ—Å—è–≥–Ω—É—Ç–æ max_comments={max_comments}")
                    break

            # –ª–æ–≥ –ø–æ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ
            dt_page = time.perf_counter() - t_page
            logger.info(f"üìÑ Page {page}: +{fetched_this_page} comments, {dt_page:.2f}s")

            if max_comments is not None and len(rows) >= max_comments:
                break

            next_page_token = response.get("nextPageToken")
            if not next_page_token:
                break

            if sleep_between_pages > 0:
                time.sleep(sleep_between_pages)

    except HttpError as e:
        logger.error(f"‚ùå YouTube API error: {e}")
    except Exception as e:
        logger.error(f"‚ùå Unexpected error: {e}")

    # –î–æ DataFrame
    df = pd.DataFrame(rows, columns=[
        "video_id","comment_id","parent_id","author","author_channel_id","text",
        "like_count","reply_count","published_at","updated_at","is_reply","fetched_at"
    ])

    # (–û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ) –∫–µ—à —É SQLite
    if sqlite_path:
        try:
            with sqlite3.connect(sqlite_path) as conn:
                _ensure_sqlite(conn)
                _upsert_comments(conn, df.to_dict(orient="records"))
            logger.info(f"üíæ Saved {len(df)} comments into SQLite: {sqlite_path}")
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Failed to write SQLite cache: {e}")

    total_dt = time.perf_counter() - t0
    logger.info(f"‚úÖ Done. fetched={len(df)} in {total_dt:.2f}s (video_id={video_id})")
    return df


# ---------- –ü—Ä–æ—Å—Ç–∏–π CLI-—Ç–µ—Å—Ç ----------
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Fetch YouTube comments")
    parser.add_argument("url_or_id", help="YouTube URL or video_id")
    parser.add_argument("--api_key", default=os.getenv("YOUTUBE_API_KEY"))
    parser.add_argument("--order", default="relevance", choices=["relevance", "time"])
    parser.add_argument("--include_replies", action="store_true")
    parser.add_argument("--max_pages", type=int, default=None)
    parser.add_argument("--max_comments", type=int, default=2000)
    parser.add_argument("--sqlite", type=str, default=None)
    args = parser.parse_args()

    df = fetch_comments(
        args.url_or_id,
        api_key=args.api_key,
        order=args.order,
        include_replies=args.include_replies,
        max_comments=args.max_comments,
        max_pages=args.max_pages,
        sqlite_path=args.sqlite,
    )
    print(df.head())
    print(f"Total: {len(df)}")
