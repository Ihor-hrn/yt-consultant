# app/tools/analyze_video_tool.py
# -*- coding: utf-8 -*-
"""
–í–∏—Å–æ–∫–æ—Ä—ñ–≤–Ω–µ–≤–∏–π —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É YouTube –≤—ñ–¥–µ–æ –¥–ª—è Telegram –±–æ—Ç–∞.
–û–±'—î–¥–Ω—É—î –≤–µ—Å—å –ø–∞–π–ø–ª–∞–π–Ω: –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è ‚Üí –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥ ‚Üí –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è ‚Üí –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è.
"""

from __future__ import annotations
import os
import time
from typing import Dict, Any, Optional, List
import pandas as pd

# –Ü–º–ø–æ—Ä—Ç–∏ –Ω–∞—à–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω—É
from .youtube import fetch_comments, extract_video_id
from .preprocess import select_fast_batch, preprocess_comments_df
from .topics_taxonomy import TAXONOMY, ID2NAME
from .topics_llm import classify_llm_full, aggregate_topics
from .classification_db import save_analysis_to_db, get_latest_analysis_data

try:
    from logger import logger
except Exception:
    import logging
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")
    logger = logging.getLogger("analyze_video_tool")

def aggregate_sentiment(df_classified: pd.DataFrame) -> pd.DataFrame:
    """–û–±—á–∏—Å–ª—é—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ."""
    if df_classified.empty or "sentiment" not in df_classified.columns:
        return pd.DataFrame(columns=["sentiment", "count", "share"])
    
    # –†–∞—Ö—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–∂–Ω–æ—ó —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ
    sentiment_counts = df_classified["sentiment"].value_counts()
    total = len(df_classified)
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ DataFrame –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    result = []
    for sentiment in ["positive", "neutral", "negative"]:
        count = sentiment_counts.get(sentiment, 0)
        share = count / total if total > 0 else 0.0
        result.append({
            "sentiment": sentiment,
            "count": count,
            "share": share
        })
    
    return pd.DataFrame(result)

def analyze_video_tool(
    url_or_id: str,
    *,
    limit: int = 1200,
    sqlite_path: str = "./.cache.db",
    fast_mode: bool = True,
    force_reanalyze: bool = False
) -> Dict[str, Any]:
    """
    –ì–æ–ª–æ–≤–Ω–∏–π —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É YouTube –≤—ñ–¥–µ–æ —á–µ—Ä–µ–∑ LLM.
    
    Args:
        url_or_id: YouTube URL –∞–±–æ video_id
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        sqlite_path: –®–ª—è—Ö –¥–æ SQLite –∫–µ—à—É
        fast_mode: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —à–≤–∏–¥–∫–∏–π —Ä–µ–∂–∏–º (—Ç–æ–ø –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ –∑–∞ –ª–∞–π–∫–∞–º–∏)
        force_reanalyze: –ü—Ä–∏–º—É—Å–æ–≤–æ –ø–µ—Ä–µ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ —î –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –¥–∞–Ω—ñ
        
    Returns:
        {
            "success": bool,
            "error": str | None,
            "video_id": str,
            "analysis_id": int,
            "stats": {
                "total_fetched": int,
                "used_for_analysis": int,
                "classified": int
            },
            "topics": [
                {
                    "topic_id": str,
                    "name": str, 
                    "count": int,
                    "share": float,
                    "top_quote": str
                }
            ],
            "processing_time": float
        }
    """
    start_time = time.perf_counter()
    
    try:
        # 1. –í–∏—Ç—è–≥–∞—î–º–æ video_id
        video_id = extract_video_id(url_or_id)
        if not video_id:
            return {
                "success": False,
                "error": "–ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏—Ç—è–≥—Ç–∏ video_id –∑ URL",
                "video_id": None
            }
        
        logger.info(f"üé¨ –ü–æ—á–∏–Ω–∞—î–º–æ –∞–Ω–∞–ª—ñ–∑ –≤—ñ–¥–µ–æ: {video_id} (limit={limit}, fast_mode={fast_mode})")
        
        # 2. –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î –∑–±–µ—Ä–µ–∂–µ–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        if not force_reanalyze:
            logger.info("üîç –ü–µ—Ä–µ–≤—ñ—Ä—è—é —á–∏ —î –∑–±–µ—Ä–µ–∂–µ–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏...")
            existing_data = get_latest_analysis_data(video_id, sqlite_path)
            if "error" not in existing_data and existing_data.get("topics"):
                logger.info(f"üìä –ó–Ω–∞–π–¥–µ–Ω–æ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–ª—è {video_id}")
                processing_time = time.perf_counter() - start_time
                
                return {
                    "success": True,
                    "error": None,
                    "video_id": video_id,
                    "analysis_id": existing_data["analysis_id"],
                    "stats": {
                        "total_fetched": existing_data["total_comments"],
                        "used_for_analysis": existing_data["used_comments"],
                        "classified": existing_data["used_comments"]
                    },
                    "topics": existing_data["topics"][:5],  # –¢–æ–ø-5 —Ç–µ–º
                    "sentiment": existing_data.get("sentiment", []),
                    "processing_time": processing_time,
                    "from_cache": True
                }
        
        # 3. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ
        logger.info("üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤...")
        df_all = fetch_comments(
            url_or_id,
            sqlite_path=sqlite_path,
            include_replies=True,
            max_comments=min(5000, limit * 4)  # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –±—ñ–ª—å—à–µ –¥–ª—è –≤—ñ–¥–±–æ—Ä—É
        )
        
        if df_all.empty:
            return {
                "success": False,
                "error": "–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ. –ú–æ–∂–ª–∏–≤–æ, –≤—ñ–¥–µ–æ –ø—Ä–∏–≤–∞—Ç–Ω–µ –∞–±–æ –≤—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ.",
                "video_id": video_id
            }
        
        logger.info(f"   –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {len(df_all)} –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤")
        
        # 4. –®–≤–∏–¥–∫–∏–π —Ä–µ–∂–∏–º + –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥
        logger.info("üîß –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤...")
        if fast_mode:
            df_selected = select_fast_batch(
                df_all, 
                mode="top_likes", 
                limit=limit, 
                include_replies=False
            )
        else:
            df_selected = df_all.head(limit)
        
        logger.info(f"   –í—ñ–¥—ñ–±—Ä–∞–Ω–æ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É: {len(df_selected)} –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤")
        
        df_processed = preprocess_comments_df(
            df_selected,
            min_chars=12,
            keep_langs=None  # –ó–∞–ª–∏—à–∞—î–º–æ –≤—Å—ñ –º–æ–≤–∏ –¥–ª—è LLM
        )
        
        if df_processed.empty:
            return {
                "success": False,
                "error": "–ü—ñ—Å–ª—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥—É –Ω–µ –∑–∞–ª–∏—à–∏–ª–æ—Å—è –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É",
                "video_id": video_id
            }
        
        logger.info(f"   –ü—ñ—Å–ª—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥—É: {len(df_processed)} –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤")
        
        # 5. LLM –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è
        logger.info("ü§ñ LLM –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è —á–µ—Ä–µ–∑ OpenRouter...")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ API –∫–ª—é—á
        if not os.getenv("OPENROUTER_API_KEY"):
            return {
                "success": False,
                "error": "–í—ñ–¥—Å—É—Ç–Ω—ñ–π OPENROUTER_API_KEY –≤ –∑–º—ñ–Ω–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞",
                "video_id": video_id
            }
        
        # –û–±—Ä—ñ–∑–∞—î–º–æ —Ç–µ–∫—Å—Ç–∏ –¥–æ 500 —Å–∏–º–≤–æ–ª—ñ–≤ –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó —Ç–æ–∫–µ–Ω—ñ–≤
        df_for_llm = df_processed.copy()
        df_for_llm["text_clean"] = df_for_llm["text_clean"].apply(
            lambda x: str(x)[:500] if isinstance(x, str) else ""
        )
        
        df_classified = classify_llm_full(
            df_for_llm, 
            TAXONOMY, 
            text_col="text_clean", 
            batch_size=20
        )
        
        logger.info(f"   –ö–ª–∞—Å–∏—Ñ—ñ–∫–æ–≤–∞–Ω–æ: {len(df_classified)} –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤")
        
        # 6. –ê–≥—Ä–µ–≥–∞—Ü—ñ—è —Ç–µ–º
        logger.info("üìä –ê–≥—Ä–µ–≥–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤...")
        topics_summary = aggregate_topics(df_classified)
        sentiment_summary = aggregate_sentiment(df_classified)
        
        if topics_summary.empty:
            return {
                "success": False,
                "error": "–ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –∑–≤–µ–¥–µ–Ω–Ω—è —Ç–µ–º",
                "video_id": video_id
            }
        
        # 7. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –ë–î
        logger.info("üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤...")
        model_name = os.getenv("MODEL_SUMMARY", "openai/gpt-4o-mini")
        
        analysis_id = save_analysis_to_db(
            video_id=video_id,
            total_comments=len(df_all),
            used_comments=len(df_classified),
            model_name=model_name,
            df_classified=df_classified,
            topics_summary=topics_summary,
            sqlite_path=sqlite_path,
            fast_mode=fast_mode,
            sentiment_summary=sentiment_summary
        )
        
        if analysis_id == -1:
            logger.warning("‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤ –ë–î, –∞–ª–µ –∞–Ω–∞–ª—ñ–∑ –≤–∏–∫–æ–Ω–∞–Ω–æ")
        
        # 8. –§–æ—Ä–º—É—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å
        processing_time = time.perf_counter() - start_time
        
        # –ì–æ—Ç—É—î–º–æ —Ç–æ–ø-—Ç–µ–º–∏ –∑ —Ü–∏—Ç–∞—Ç–∞–º–∏
        topics_with_quotes = []
        for _, topic_row in topics_summary.head(5).iterrows():
            topic_id = topic_row["topic_id"]
            topic_name = ID2NAME.get(topic_id, topic_id)
            
            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–π–∫—Ä–∞—â—É —Ü–∏—Ç–∞—Ç—É –¥–ª—è —Ü—ñ—î—ó —Ç–µ–º–∏
            topic_comments = df_classified[
                df_classified["topic_labels_llm"].apply(
                    lambda labels: isinstance(labels, list) and topic_id in labels
                )
            ]
            
            top_quote = ""
            if not topic_comments.empty:
                best_comment = topic_comments.sort_values(
                    ["like_count", "published_at"], 
                    ascending=[False, True]
                ).iloc[0]
                top_quote = str(best_comment.get("text_clean", ""))[:200]
            
            topics_with_quotes.append({
                "topic_id": topic_id,
                "name": topic_name,
                "count": int(topic_row["count"]),
                "share": float(topic_row["share"]),
                "top_quote": top_quote
            })
        
        logger.info(f"‚úÖ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {processing_time:.2f} —Å–µ–∫")
        
        return {
            "success": True,
            "error": None,
            "video_id": video_id,
            "analysis_id": analysis_id,
            "stats": {
                "total_fetched": len(df_all),
                "used_for_analysis": len(df_processed),
                "classified": len(df_classified)
            },
            "topics": topics_with_quotes,
            "sentiment": saved_data.get("sentiment", []),
            "processing_time": processing_time,
            "from_cache": False
        }
        
    except Exception as e:
        processing_time = time.perf_counter() - start_time
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É: {e}")
        
        return {
            "success": False,
            "error": f"–ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É: {str(e)}",
            "video_id": video_id if 'video_id' in locals() else None,
            "processing_time": processing_time
        }

def search_comments_for_qa(
    video_id: str,
    question: str,
    sqlite_path: str = "./.cache.db",
    max_results: int = 5
) -> List[Dict[str, Any]]:
    """
    –ù–∞—ó–≤–Ω–∏–π –ø–æ—à—É–∫ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –¥–ª—è Q&A —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—É.
    –®—É–∫–∞—î –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ, —â–æ –º—ñ—Å—Ç—è—Ç—å –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –∑ –ø–∏—Ç–∞–Ω–Ω—è.
    
    Args:
        video_id: ID YouTube –≤—ñ–¥–µ–æ
        question: –ü–∏—Ç–∞–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
        sqlite_path: –®–ª—è—Ö –¥–æ SQLite –∫–µ—à—É
        max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        
    Returns:
        [
            {
                "comment_id": str,
                "text": str,
                "author": str,
                "like_count": int,
                "relevance_score": float
            }
        ]
    """
    try:
        import sqlite3
        import re
        from collections import Counter
        
        # –í–∏—Ç—è–≥–∞—î–º–æ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –∑ –ø–∏—Ç–∞–Ω–Ω—è
        question_lower = question.lower()
        # –ü—Ä–∏–±–∏—Ä–∞—î–º–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ —Ç–∞ –∫–æ—Ä–æ—Ç–∫—ñ —Å–ª–æ–≤–∞
        stop_words = {"—â–æ", "—è–∫", "–¥–µ", "–∫–æ–ª–∏", "—á–æ–º—É", "—á–∏", "—ñ", "–≤", "–Ω–∞", "–∑", "–¥–ª—è", "–ø—Ä–æ", "–∞–±–æ", "—Ç–∞", "–∞–ª–µ"}
        words = re.findall(r'\b\w{3,}\b', question_lower)
        keywords = [w for w in words if w not in stop_words][:10]  # –ú–∞–∫—Å–∏–º—É–º 10 –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
        
        if not keywords:
            return []
        
        logger.info(f"üîç –ü–æ—à—É–∫ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –¥–ª—è –ø–∏—Ç–∞–Ω–Ω—è: {question[:50]}...")
        logger.info(f"   –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞: {keywords}")
        
        with sqlite3.connect(sqlite_path) as conn:
            # –û—Ç—Ä–∏–º—É—î–º–æ –≤—Å—ñ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ –¥–ª—è –≤—ñ–¥–µ–æ
            query = """
                SELECT 
                    c.comment_id, c.text, c.author, c.like_count, c.published_at
                FROM comments c
                WHERE c.video_id = ?
                ORDER BY c.like_count DESC
            """
            
            results = []
            cursor = conn.execute(query, [video_id])
            
            for row in cursor.fetchall():
                comment_id, text, author, like_count, published_at = row
                text_lower = str(text or "").lower()
                
                # –†–∞—Ö—É—î–º–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å
                relevance_score = 0
                for keyword in keywords:
                    if keyword in text_lower:
                        relevance_score += 1
                        # –ë–æ–Ω—É—Å –∑–∞ —Ç–æ—á–Ω–∏–π –∑–±—ñ–≥
                        if keyword == text_lower or f" {keyword} " in text_lower:
                            relevance_score += 0.5
                
                # –î–æ–¥–∞—î–º–æ –±–æ–Ω—É—Å –∑–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ª–∞–π–∫—ñ–≤
                like_bonus = min(like_count / 100, 1.0) if like_count > 0 else 0
                relevance_score += like_bonus
                
                if relevance_score > 0:
                    results.append({
                        "comment_id": comment_id,
                        "text": str(text or ""),
                        "author": str(author or ""),
                        "like_count": int(like_count or 0),
                        "relevance_score": relevance_score
                    })
            
            # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—é —Ç–∞ –ª–∞–π–∫–∞–º–∏
            results.sort(key=lambda x: (x["relevance_score"], x["like_count"]), reverse=True)
            
            logger.info(f"   –ó–Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤")
            return results[:max_results]
        
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤: {e}")
        return []
